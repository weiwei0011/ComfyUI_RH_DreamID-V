# Copyright 2024-2025 Bytedance Ltd. and/or its affiliates. All rights reserved.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from tqdm import tqdm
import os
import cv2
import csv
import json
import math
import random
import numpy as np
import imageio
import matplotlib
import matplotlib.pyplot as plt
from glob import glob
from PIL import Image, ImageSequence
from io import BytesIO
from IPython.display import Video
from IPython.display import display, Image as IPyImage
import torchvision.transforms as T

import sys
from .media_pipe.mp_utils import LMKExtractor
from .media_pipe.draw_util import FaceMeshVisualizer
from .media_pipe.pose_util import project_points_with_trans, matrix_to_euler_and_translation, euler_and_translation_to_matrix

# Try to import hybrid detector (InsightFace + MediaPipe)
HYBRID_AVAILABLE = False
try:
    from .media_pipe.face_detector_hybrid import HybridLMKExtractor, INSIGHTFACE_AVAILABLE
    HYBRID_AVAILABLE = INSIGHTFACE_AVAILABLE
    if HYBRID_AVAILABLE:
        print("[get_video_npy] InsightFace hybrid detector available")
except ImportError as e:
    print(f"[get_video_npy] Hybrid detector not available: {e}")

# Default global extractor for backward compatibility (used by get_video_npy)
# Uses default threshold of 0.5
lmk_extractor = LMKExtractor(min_detection_confidence=0.5, min_tracking_confidence=0.5)
vis = FaceMeshVisualizer(forehead_edge=False)

def prehandle_video(video_path, save_path=None, fps=24, debug=False, min_detection_confidence=0.3, use_insightface=False):
    """
    Detect faces in video and return face detection results.
    For frames without detectable faces, use the previous frame's result (interpolation).
    
    NOTE: This function NO LONGER re-encodes the video. The original video is used directly.
    The save_path parameter is kept for backward compatibility but ignored.
    
    Args:
        video_path: Path to input video
        save_path: DEPRECATED - kept for backward compatibility, ignored
        fps: Frames per second (not used, kept for backward compatibility)
        debug: Enable debug logging
        min_detection_confidence: Face detection threshold (only used if use_insightface=True)
        use_insightface: Use InsightFace + MediaPipe hybrid detection (default: False to match original behavior)
    
    Returns:
        interpolated_frames: list of frame indices that used interpolated face results
        face_results: list of face detection results for ALL frames (with interpolation)
    """
    # Use original LMKExtractor with default parameters (matches original project behavior)
    # Only use hybrid detector if explicitly requested
    if use_insightface and HYBRID_AVAILABLE:
        from .media_pipe.face_detector_hybrid import HybridLMKExtractor
        extractor = HybridLMKExtractor(
            min_detection_confidence=min_detection_confidence,
            min_tracking_confidence=min_detection_confidence,
            use_insightface=True,
            insightface_det_thresh=min_detection_confidence
        )
        print(f"[prehandle_video] Using InsightFace + MediaPipe hybrid detector (threshold: {min_detection_confidence})")
    else:
        # Use original LMKExtractor with DEFAULT parameters (no custom thresholds)
        # This matches the original project behavior exactly
        extractor = LMKExtractor()  # No parameters = use MediaPipe defaults
        print(f"[prehandle_video] Using original MediaPipe detector (default thresholds)")
    
    frames = imageio.get_reader(video_path)
    
    face_results = []  # Store face results for ALL frames
    interpolated_frames = []  # Track frames that used interpolation
    last_valid_result = None  # Store last valid face result for interpolation
    total_frames = 0
    detected_count = 0
    
    # Only enable debug for first few frames to avoid log spam
    debug_limit = 10 if debug else 0
    
    for i, frame in enumerate(frames):
        total_frames += 1
        frame_bgr = cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR)
        face_result = lmk_extractor(frame_bgr)
        if face_result is None:
            print(f'frame {i} no face detected')
            skip_frames_index.append(i)
            skip_frames_data[i] = frame
            continue
        writer.append_data(frame)
        # print(f'frame {i} done')
    writer.close()
    return skip_frames_index, skip_frames_data

def get_video_npy(video_path):
    """
    Extract face landmarks from video frames.
    Frames without detectable faces are skipped.
    """
    frames = imageio.get_reader(video_path)

    face_results = []
    skip_frames_index = []
    skip_frames_data = {}
    detected_frames = []

    frame_width = 0
    frame_height = 0
    
    for i, frame in enumerate(frames):
        # if i == 0:
        #     frame_width = frame.shape[1]
        #     frame_height = frame.shape[0]
        #     print(f'frame size: {frame_width}x{frame_height}')
        #     while frame_width < 720 or frame_height < 720:
        #         frame_width *= 2
        #         frame_height *= 2
        #     print(f'frame resize to: {frame_width}x{frame_height}')
        # if frame_width != frame.shape[1]:
        #     frame = cv2.resize(frame, (frame_width, frame_height), interpolation=cv2.INTER_CUBIC)

        # print(f'frame size: {frame.shape[1]}x{frame.shape[0]}')

        frame_bgr = cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR)
        
        face_result = lmk_extractor(frame_bgr)
        # assert face_result is not None, "Can not detect a face in the reference image."
        if face_result is None:
            print(f'frame {i} no face detected')
            skip_frames_index.append(i)
            skip_frames_data[i] = frame
            continue
        face_result['width'] = frame_bgr.shape[1]
        face_result['height'] = frame_bgr.shape[0]
        
        face_results.append(face_result)
        detected_frames.append(frame)
    return face_results, skip_frames_index, skip_frames_data, detected_frames
    


if __name__ == '__main__':
    video_path = '/mnt/bn/hmbytenas/code/guoxu/guoxu/video_face_swap_test/videos/male_005.mp4'
    npy_root_path = '/mnt/bn/hmbytenas/code/guoxu/guoxu/video_face_swap_test/video_npy'
    face_results = get_video_npy(video_path)
    video_name = os.path.basename(video_path).split('.')[0]
    save_path = os.path.join(npy_root_path, f'{video_name}.npy')
    np.save(save_path, face_results)
    print(save_path, 'done')
